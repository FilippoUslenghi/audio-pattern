@inproceedings{esc-50,
  title = {{ESC}: {Dataset} for {Environmental Sound Classification}},
  author = {Piczak, Karol J.},
  booktitle = {Proceedings of the 23rd {Annual ACM Conference} on {Multimedia}},
  date = {2015-10-13},
  url = {http://dl.acm.org/citation.cfm?doid=2733373.2806390},
  doi = {10.1145/2733373.2806390},
  location = {{Brisbane, Australia}},
  isbn = {978-1-4503-3459-4},
  publisher = {{ACM Press}},
  pages = {1015--1018},
}

@misc{hwang2023torchaudio,
   title={TorchAudio 2.1: Advancing speech recognition, self-supervised learning, and audio processing components for PyTorch},
   author={Jeff Hwang and Moto Hira and Caroline Chen and Xiaohui Zhang and Zhaoheng Ni and Guangzhi Sun and Pingchuan Ma and Ruizhe Huang and Vineel Pratap and Yuekai Zhang and Anurag Kumar and Chin-Yun Yu and Chuang Zhu and Chunxi Liu and Jacob Kahn and Mirco Ravanelli and Peng Sun and Shinji Watanabe and Yangyang Shi and Yumeng Tao and Robin Scheibler and Samuele Cornell and Sean Kim and Stavros Petridis},
   year={2023},
   eprint={2310.17864},
   archivePrefix={arXiv},
   primaryClass={eess.AS}
}

@article{yang2021torchaudio,
  title={TorchAudio: Building Blocks for Audio and Speech Processing},
  author={Yao-Yuan Yang and Moto Hira and Zhaoheng Ni and
          Anjali Chourdia and Artyom Astafurov and Caroline Chen and
          Ching-Feng Yeh and Christian Puhrsch and David Pollack and
          Dmitriy Genzel and Donny Greenberg and Edward Z. Yang and
          Jason Lian and Jay Mahadeokar and Jeff Hwang and Ji Chen and
          Peter Goldsborough and Prabhat Roy and Sean Narenthiran and
          Shinji Watanabe and Soumith Chintala and
          Vincent Quenneville-Bélair and Yangyang Shi},
  journal={arXiv preprint arXiv:2110.15018},
  year={2021}
}

@article{BANSAL2022200115,
title = {Environmental Sound Classification: A descriptive review of the literature},
journal = {Intelligent Systems with Applications},
volume = {16},
pages = {200115},
year = {2022},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2022.200115},
url = {https://www.sciencedirect.com/science/article/pii/S2667305322000539},
author = {Anam Bansal and Naresh Kumar Garg},
keywords = {Environmental Sound Classification, Feature extraction, Feature selection, Machine learning classifiers, Deep neural networks},
abstract = {Automatic environmental sound classification (ESC) is one of the upcoming areas of research as most of the traditional studies are focused on speech and music signals. Classifying environmental sounds such as glass breaking, helicopter, baby crying and many more can aid in surveillance systems as well as criminal investigations. In this paper, a vast range of literature in the field of ESC is elucidated from various facets like preprocessing, feature extraction, and classification techniques. Researchers have used various noise removal and signal enhancement techniques to preprocess the signals. This paper explicates multitude of datasets used in recent studies along with the year of publication and maximum accuracy achieved with the dataset. Deep Neural Networks surpass the traditional machine learning classifiers. The future challenges and prospective research in this field are proposed. Since no recent review on ESC has been published, this study will open up novel ways for certain business applications and security systems.}
}

@INPROCEEDINGS{chu,

  author={Chu, Selina and Narayanan, Shrikanth and Kuo, C.-c. Jay and Mataric, Maja J.},

  booktitle={2006 IEEE International Conference on Multimedia and Expo}, 

  title={Where am I? Scene Recognition for Mobile Robots using Audio Features}, 

  year={2006},

  volume={},

  number={},

  pages={885-888},

  keywords={Layout;Mobile robots;Navigation;Robot sensing systems;Character recognition;Robot vision systems;Robotics and automation;Speech analysis;Computer science;Spectral analysis},

  doi={10.1109/ICME.2006.262661}
}

@InProceedings{theodorus,
author="Theodorou, Theodoros
and Mporas, Iosif
and Fakotakis, Nikos",
editor="Ronzhin, Andrey
and Potapova, Rodmonga
and Fakotakis, Nikos",
title="Automatic Sound Recognition of Urban Environment Events",
booktitle="Speech and Computer",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="129--136",
abstract="The audio analysis of speaker's surroundings has been a first step for several processing systems that enable speaker's mobility though his daily life. These algorithms usually operate in a short-time analysis decomposing the incoming events in time and frequency domain. In this paper, an automatic sound recognizer is studied, which investigates audio events of interest from urban environment. Our experiments were conducted using a close set of audio events from which well known and commonly used audio descriptors were extracted and models were training using powerful machine learning algorithms. The best urban sound recognition performance was achieved by SVMs with accuracy equal to approximately 93 {\%}.",
isbn="978-3-319-23132-7"
}

@INPROCEEDINGS{zhang,

  author={Zhang, Xiaohu and Zou, Yuexian and Shi, Wei},

  booktitle={2017 22nd International Conference on Digital Signal Processing (DSP)}, 

  title={Dilated convolution neural network with LeakyReLU for environmental sound classification}, 

  year={2017},

  volume={},

  number={},

  pages={1-5},

  keywords={Convolution;Feature extraction;Training;Spectrogram;Neural networks;Information filters;Environmental sound classification;Dilated Convolution Neural Network;Leaky Rectified Linear Unit;Activation Function},

  doi={10.1109/ICDSP.2017.8096153}
}

@inproceedings{Bountourakis,
author = {Bountourakis, Vasileios and Vrysis, Lazaros and Papanikolaou, George},
title = {Machine Learning Algorithms for Environmental Sound Recognition: Towards Soundscape Semantics},
year = {2015},
isbn = {9781450338967},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814895.2814905},
doi = {10.1145/2814895.2814905},
abstract = {This paper investigates methods aiming at the automatic recognition and classification of discrete environmental sounds, for the purpose of subsequently applying these methods to the recognition of soundscapes. Research in audio recognition has traditionally focused on the domains of speech and music. Comparatively little research has been done towards recognizing non-speech environmental sounds. For this reason, in this paper, we apply existing techniques that have been proved efficient in the other two domains. These techniques are comprehensively compared to determine the most appropriate one for addressing the problem of environmental sound recognition.},
booktitle = {Proceedings of the Audio Mostly 2015 on Interaction With Sound},
articleno = {5},
numpages = {7},
keywords = {Environmental Sound Recognition, audio classification, computer audition, feature extraction, feature selection, machine learning algorithms, semantic audio analysis},
location = {Thessaloniki, Greece},
series = {AM '15}
}

@inproceedings{Ntalampiras,
  title={Automatic Recognition of Urban Environmental Sound Events},
  author={Stavros Ntalampiras and Ilyas Potamitis and Nikos Fakotakis},
  year={2008},
  url={https://api.semanticscholar.org/CorpusID:11792727}
}

@article{zhan,
author = {Zhan, Yi and Kuroda, Tadahiro},
year = {2012},
month = {02},
pages = {},
title = {Wearable sensor-based human activity recognition from environmental background sounds},
volume = {5},
journal = {Journal of Ambient Intelligence and Humanized Computing},
doi = {10.1007/s12652-012-0122-2}
}